{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smal3iutre2/washu-5509/blob/main/HW/CSE5509_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFpgp_rIA_TY"
      },
      "source": [
        "# Land Use and Land Cover Classification using Pytorch\n",
        "\n",
        "\n",
        "In this assignment, you will learn how to:\n",
        "- Train a deep learning model for Image classification using Pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gchK901cq3hN"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "\n",
        "*   [Overview](#overview)\n",
        "*   [Software Requirements](#software-requirements)\n",
        "*   [Data Description](#data-description)\n",
        "*   [Methodology](#methodology)\n",
        "*   [Results & Discussion](#results-and-discussion)\n",
        "*   [Credits](#credits)\n",
        "*   [References](#references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nuv5XES8Ejc_"
      },
      "source": [
        "<a name=\"overview\"></a>\n",
        "# Overview\n",
        "This tutorial covers an introduction to image classification using Pytorch for land use and land cover (LULC) mapping.\n",
        "\n",
        "Specifically, you will learn how to:\n",
        "- Classify satellite images into 10 LULC categories using the [EuroSAT dataset](https://arxiv.org/abs/1709.00029)\n",
        "- Design different CNN models and tune for various hyperparameters\n",
        "- Fine-tune a Resnet-18 CNN model for image classification\n",
        "- Save and load trained models in Pytorch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMhFV8gzrqO-"
      },
      "source": [
        "<a name=\"software-requirements\"></a>\n",
        "# Software Requirements\n",
        "\n",
        "This notebook requires Python >= 3.7. The following libraries are required:\n",
        "*   tqdm\n",
        "*   pandas\n",
        "*   numpy\n",
        "*   matplotlib\n",
        "*   pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H08M_-5iEbc1"
      },
      "source": [
        "## Enabling GPU in Google Colab\n",
        "Before we start, you will need access to a GPU.  Fortunately, Google Colab provides free access to computing resources including GPUs. The GPUs currently available in Colab include Nvidia K80s, T4s, P4s and P100s. Unfortunately, there is no way to choose what type of GPU you can connect to in Colab. [See here for information](https://research.google.com/colaboratory/faq.html#gpu-availability).\n",
        "\n",
        "To enable GPU in Google Colab:\n",
        "1. Navigate to Edit→Notebook Settings or Runtime→Change Runtime Type.\n",
        "2. Select GPU from the Hardware Accelerator drop-down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw688STi6Z6k"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import ssl\n",
        "import urllib.request\n",
        "import zipfile\n",
        "# Standard libraries\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Data manipulation and visualization\n",
        "import seaborn as sns\n",
        "# Deep Learning libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchsummary\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, models, transforms\n",
        "from tqdm import tqdm\n",
        "# Download EuroSAT\n",
        "from torchvision.datasets import EuroSAT\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ffVUKx7P6RD"
      },
      "source": [
        "## Determine Torch Device\n",
        "If you are running in Google Colab, check that the GPU is enabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPzhNed7P2i9"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    else:\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "device = get_device()\n",
        "\n",
        "print(f\"Using torch device type: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIQAlBV9GbCZ"
      },
      "source": [
        "<a name=\"data-description\"></a>\n",
        "# Data Description\n",
        "\n",
        "In this section, you will learn how to:\n",
        "- Download the EuroSAT dataset into your Google Drive\n",
        "- Generate the train, validation and test sets by splitting the EuroSAT dataset\n",
        "- Visualize a sample of the images and their LULC labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK0AM2bWGxsw"
      },
      "source": [
        "## Download EuroSAT Dataset\n",
        "The [EuroSAT dataset](https://github.com/phelber/EuroSAT) contains 27,000 labelled 64x64 pixel Sentinel-2 satellite image patches with 10 different LULC categories. Both RGB and multi-spectral (MS) images are available for download. For simplicity, we will focus on RGB image classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GNyqemwGwr9"
      },
      "outputs": [],
      "source": [
        "# Define the root directory for the dataset\n",
        "root_dir = './data'\n",
        "\n",
        "# Instantiate the EuroSAT dataset\n",
        "dataset = EuroSAT(root=root_dir, download=True)\n",
        "\n",
        "# Check dataset size and structure\n",
        "print(f\"Number of samples: {len(dataset)}\")\n",
        "print(f\"Classes: {dataset.classes}\")\n",
        "\n",
        "# Example: Access the first sample\n",
        "sample = dataset[0]\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9bbA-_d-Vis"
      },
      "source": [
        "### Data Augmentation\n",
        "\n",
        "Data augmentation is a  technique that randomly applies image transformations, e.g. crops, horizontal flips, and vertical flips, to the input images during model training. These perturbations reduce the neural network's overfitting to the training dataset, and they allow it to generalize better to the unseen test dataset.\n",
        "<br><br>\n",
        "<center> <img src=\"https://www.researchgate.net/publication/319413978/figure/fig2/AS:533727585333249@1504261980375/Data-augmentation-using-semantic-preserving-transformation-for-SBIR.png\" width=\"400\"/>\n",
        "</center>\n",
        "<br>\n",
        "<font size=2>Image Source: Ahmad, Jamil & Muhammad, Khan & Baik, Sung. (2017). Data augmentation-assisted deep learning of hand-drawn partially colored sketches for visual search. PLOS ONE. 12. e0183838. 10.1371/journal.pone.0183838. </font>\n",
        "<br>\n",
        "\n",
        "\n",
        "### Image Normalization\n",
        "Additionally, in the cell below, the `transforms.Normalize` method normalizes each of the three channels to the given means and standard deviations defined in the `imagenet_mean` and `imagenet_std` variables. ImageNet is a large training dataset of images and labels.  Later in this tutorial, we will be using a model pre-trained on this dataset.  In order to use this pre-trained model for our LULC dataset, we need to ensure that the input dataset is normalized to have the same statistics (mean and standard deviation) as ImageNet.\n",
        "\n",
        "<br>\n",
        "<center> <img src=\"https://cv.gluon.ai/_images/imagenet_banner.jpeg\" width=\"400\"/>\n",
        "<br>\n",
        "<font size=2>Image Source: https://cv.gluon.ai/build/examples_datasets/imagenet.html </font>\n",
        "<br>\n",
        "</center>\n",
        "\n",
        "Existing research has found that using models pretrained on massive datasets, such as ImageNet, improves accuracy when applying these neural networks to new datasets.  Pre-trained models serve as excellent generic feature extractors.  [Please read here for more information](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osNVdWjtIQMK"
      },
      "outputs": [],
      "source": [
        "imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "def get_transforms(transforms_type='imagenet'):\n",
        "    if transforms_type == 'imagenet':\n",
        "        input_size = 224\n",
        "    else:\n",
        "        input_size = 64\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(imagenet_mean, imagenet_std)\n",
        "    ])\n",
        "\n",
        "    valid_transform = transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(imagenet_mean, imagenet_std)\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(imagenet_mean, imagenet_std)\n",
        "    ])\n",
        "\n",
        "    return train_transform, valid_transform, test_transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-H1Y1rHVz8t"
      },
      "source": [
        "### Load EuroSAT Dataset\n",
        "Let's start by loading the EuroSAT dataset using torch's `ImageFolder` class.\n",
        "\n",
        "`ImageFolder` is a generic data loader where the images are arranged in this way:\n",
        "\n",
        "```\n",
        "    data\n",
        "    └───AnnualCrop\n",
        "    │   │   AnnualCrop_1.jpg\n",
        "    │   │   AnnualCrop_2.jpg\n",
        "    │   │   AnnualCrop_3.jpg\n",
        "    │   │   ...\n",
        "    └───Forest\n",
        "    │   │   Forest_1.jpg\n",
        "    │   │   Forest_2.jpg\n",
        "    │   │   Forest_3.jpg\n",
        "    │   │   ...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "TM7hyGz3Ry8e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get LULC categories\n",
        "class_names = dataset.classes\n",
        "print(\"Class names: {}\".format(class_names))\n",
        "print(\"Total number of classes: {}\".format(len(class_names)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY8oTO459LXo",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Split into Train, Validation and Test Sets\n",
        "Here, we split the dataset into a train set, validation set and test set. The training set will be 70% of the Eurosat dataset, randomly selected. The valdation set will be 10% of the Eurosat dataset, randomly selected. The remaining 20% of the dataset will be our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6_Jm_Vt89-L"
      },
      "outputs": [],
      "source": [
        "def data_prep(batch_size=32,num_workers=2,transforms_type='original'):\n",
        "\n",
        "    train_transform, valid_transform, test_transform = get_transforms(transforms_type='original')\n",
        "    # Apply different transformations to the training, validation and test sets\n",
        "    train_data = EuroSAT(root=root_dir, transform=train_transform)\n",
        "    validation_data = EuroSAT(root=root_dir, transform=valid_transform)\n",
        "    test_data = EuroSAT(root=root_dir, transform=test_transform)\n",
        "\n",
        "    # Randomly split the dataset into 70% train/ 10% validation / 20% test\n",
        "    # by subsetting the transformed train and test datasets\n",
        "    indices = list(range(int(len(dataset))))\n",
        "    split1 = int(0.7 * len(dataset))\n",
        "    split2 = int(0.8 * len(dataset))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_data = data.Subset(train_data, indices=indices[:split1])\n",
        "    validation_data = data.Subset(validation_data, indices=indices[split1:split2])\n",
        "    test_data = data.Subset(test_data, indices=indices[split2:])\n",
        "\n",
        "    train_loader = data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "    valid_loader = data.DataLoader(validation_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "    test_loader = data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTD1GGkR8Ss4"
      },
      "source": [
        "Finally, we use `torch`'s `DataLoader` class to create a dataloader.  The dataloader manages fetching samples from the datasets (it can even fetch them in parallel using `num_workers`) and assembles batches of the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhoykOhv8LdN"
      },
      "outputs": [],
      "source": [
        "num_workers = 2\n",
        "batch_size = 32\n",
        "transforms_type='original'\n",
        "train_loader, valid_loader, test_loader = data_prep(batch_size=batch_size,num_workers=num_workers,transforms_type=transforms_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcjHBMnAawgS"
      },
      "source": [
        "## Visualize Data\n",
        "\n",
        "In the cell below, we will visualize a batch of the dataset.  The cell visualizes the input to the neural network (the RGB image) along with the associated label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qphYetp0ayIZ"
      },
      "outputs": [],
      "source": [
        "n = 4\n",
        "inputs, classes = next(iter(train_loader))\n",
        "fig, axes = plt.subplots(n, n, figsize=(8,8))\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        image = inputs[i * n + j].numpy().transpose((1, 2, 0))\n",
        "        image = np.clip(np.array(imagenet_std) * image + np.array(imagenet_mean), 0, 1)\n",
        "\n",
        "        title = class_names[classes[i * n + j]]\n",
        "        axes[i, j].imshow(image)\n",
        "        axes[i, j].set_title(title)\n",
        "        axes[i, j].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O-ZKD5KqdMm"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "Next, let's explore our dataset a little bit more.  In particular, how many images of each class are included?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhHebM9-qiAv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "hist = sns.histplot(dataset.targets)\n",
        "\n",
        "hist.set_xticks(range(len(dataset.classes)))\n",
        "hist.set_xticklabels(dataset.classes, rotation=90)\n",
        "hist.set_title('Histogram of Dataset Classes in EuroSAT Dataset')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6hah_GourET"
      },
      "source": [
        "# Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAa1fnmm2lI2"
      },
      "source": [
        "Below is a simple baseline network similar to the one used in [this pytorch tutorial.](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPgmTKqr8Jm9"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWAm3iuIw8j4"
      },
      "outputs": [],
      "source": [
        "# Baseline: A simple ConvNet baseline\n",
        "class baseline(nn.Module):\n",
        "    def __init__(self,hparams):\n",
        "        super().__init__()\n",
        "        self.hparams = hparams\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        if self.hparams['pooling'] == 'maxpool':\n",
        "            self.pool = nn.MaxPool2d(2, 2)\n",
        "        elif self.hparams['pooling'] == 'avgpool':\n",
        "            self.pool = nn.AvgPool2d(2, 2)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(2704, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        if self.hparams['activation'] == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif self.hparams['activation'] == 'leakyrelu':\n",
        "            self.activation = nn.LeakyReLU()\n",
        "        elif self.hparams['activation'] == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.activation(self.conv1(x)))\n",
        "        x = self.pool(self.activation(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "hparams ={'activation':'relu','pooling':'maxpool'}\n",
        "model = baseline(hparams).to(device)\n",
        "torchsummary.summary(model, (3, 64, 64), device=str(device))\n",
        "\n",
        "inputs = torch.rand(2,3,64,64, device=device)\n",
        "model(inputs).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcfJK_H4YWej"
      },
      "source": [
        "# Model Development\n",
        "Based on the specification provided, design a set of your own models with varying size and architectural choices.\n",
        "\n",
        "Architectural choices:\n",
        "*   (in_channels, out_channels, kernel_size, stride) for your convolution layers\n",
        "*  activation function choices:[ReLU, LeakyReLU, Tanh]\n",
        "*  pooling layers: [AvgPool2d, MaxPool2d]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "XPe3wm_t7OWt"
      },
      "source": [
        "## Task 1: Model 1\n",
        "This model should have 3 2D-convolution layers and maximum of 2 hidden linear layers. Feel free to choose suitable parameters for 2D-conv and linear layers.\n",
        "\n",
        "_Points:_ 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ecH4eUl7GUz",
        "tags": [
          "otter_answer_cell"
        ]
      },
      "outputs": [],
      "source": [
        "# Design your model1 based on hparams1 = {'activation':'', 'pooling':''}.\n",
        "class model1(nn.Module):\n",
        "    def __init__(self,hparams1=None):\n",
        "        super().__init__()\n",
        "        ######## CODE BELOW ############\n",
        "        ...\n",
        "        ######## CODE ABOVE ############\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ####### CODE BELOW ############\n",
        "        ...\n",
        "        ####### CODE ABOVE ############\n",
        "\n",
        "        return x\n",
        "\n",
        "hparams1 = {'activation': 'relu', 'pooling': 'maxpool'}\n",
        "# model1 = model1(hparams1).to(device)\n",
        "# torchsummary.summary(model1, (3, 64, 64))\n",
        "# inputs = torch.rand(2,3,64,64).to(device)\n",
        "# model(inputs).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "0vyrSIZ1rpR9"
      },
      "outputs": [],
      "source": [
        "def test1_public(hparams1):\n",
        "    model = model1(hparams1)\n",
        "    dummy_input = torch.randn(4, 3, 64, 64)\n",
        "    output = model(dummy_input)\n",
        "    assert output.shape == (4, 10), f\"Expected output shape (4, 10), got {output.shape}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "FhECZBHhERPM"
      },
      "source": [
        "## Task 2: Model 2\n",
        "This model should have 4 to 8 2D-convolution layers and 2 to 5 hidden linear layers. Feel free to choose suitable parameters for 2D-conv and linear layers.\n",
        "\n",
        "_Points:_ 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0ZlI7D_EYEO",
        "tags": [
          "otter_answer_cell"
        ]
      },
      "outputs": [],
      "source": [
        "# Design your model2 based on hparams2 = {'activation':'', 'pooling':''}.\n",
        "class model2(nn.Module):\n",
        "    def __init__(self,hparams2=None):\n",
        "        super().__init__()\n",
        "        ######## CODE BELOW ############\n",
        "        ...\n",
        "        ######## CODE ABOVE ############\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ####### CODE BELOW ############\n",
        "        ...\n",
        "        ####### CODE ABOVE ############\n",
        "\n",
        "        return x\n",
        "\n",
        "# hparams2 = {'activation': 'relu', 'pooling': 'maxpool'}\n",
        "# model2 = model2(hparams2).to(device)\n",
        "# torchsummary.summary(model2, (3, 64, 64))\n",
        "# inputs = torch.rand(2,3,64,64).to(device)\n",
        "# model(inputs).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5cojKhZrpR9"
      },
      "outputs": [],
      "source": [
        "def test2_public():\n",
        "    hparams2 = {'activation': 'relu', 'pooling': 'maxpool'}\n",
        "    model = model2(hparams2)\n",
        "    dummy_input = torch.randn(4, 3, 64, 64)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(dummy_input)\n",
        "    assert output.shape == (4, 10), f'Expected output shape (4, 10), got {output.shape}'\n",
        "\n",
        "test2_public()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPXbNl_TLHQQ"
      },
      "outputs": [],
      "source": [
        "def get_resnet18():\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
        "    model = model.to(device)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsCRV9m_rpR9"
      },
      "outputs": [],
      "source": [
        "def get_model(model_type='baseline', hparams=None):\n",
        "    if hparams is None:\n",
        "        hparams = {}\n",
        "\n",
        "    if not model_type == 'resnet18':\n",
        "        model_hparams = {}\n",
        "        model_hparams['activation'] = hparams['activation']\n",
        "        model_hparams['pooling'] = hparams['pooling']\n",
        "\n",
        "    if model_type == 'baseline':\n",
        "        model = baseline(model_hparams).to(device)\n",
        "        return model.to(device)\n",
        "\n",
        "    elif model_type == 'model1':\n",
        "        model = model1(hparams1=model_hparams).to(device)\n",
        "        return model.to(device)\n",
        "\n",
        "    elif model_type == 'model2':\n",
        "        model = model2(hparams2=model_hparams).to(device)\n",
        "        return model.to(device)\n",
        "\n",
        "\n",
        "    elif model_type == 'resnet18':\n",
        "        model = get_resnet18()\n",
        "        return model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gt8Da6mrpR9"
      },
      "outputs": [],
      "source": [
        "model = get_model(model_type='resnet18')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGf__UVfNNOO"
      },
      "source": [
        "## Model Training and Evaluation\n",
        "\n",
        "We can now proceed to model training and evaluation.\n",
        "\n",
        "This section has three major parts:\n",
        "\n",
        "1. Specify the criterion, optimizer, and hyperparameters (e.g. n_epochs, learning rate, etc.).\n",
        "2. Train the model on the training set by updating its weights to minimize the loss function.\n",
        "3. Evaluate the model on the test set to observe performance on new, unseen data.\n",
        "4. Repeat steps 2 and 3 `n_epochs` times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3waPRdOai7AB"
      },
      "source": [
        "### Cross Entropy Loss\n",
        "We define our loss as the cross-entropy loss, which measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. ([Source](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html))\n",
        "\n",
        "For two classes, it is computed as:\n",
        "\n",
        "$−ylog(p)-(1−y)log(1−p)$\n",
        "\n",
        "For multiclass classification with $M$ classes, it is defined as:\n",
        "\n",
        "$−\\sum_{c=1}^{M}y_{o,c}log(p_{o,c})$\n",
        "\n",
        "where\n",
        "\n",
        "- $M$ - number of classes (dog, cat, fish)\n",
        "- $log$ - the natural log\n",
        "- $y_{o,c}$ - binary indicator (0 or 1) if class label $c$ is the classification for observation $o$\n",
        "- $p_{o,c}$- predicted probability observation $o$ is of class $c$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE0eD6QyAgx6"
      },
      "outputs": [],
      "source": [
        "# Specify criterion\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE_4SGQTApFL"
      },
      "source": [
        "### Stochastic Gradient Descent\n",
        "Remember that the goal of stochastic gradient descent (SGD) is to minimize the loss function. To do this, it computes the slope (gradient) of the loss function at the current point and moves in the opposite direction of the slope towards the steepest descent.\n",
        "<center> <img src=\"https://miro.medium.com/max/1400/1*P7z2BKhd0R-9uyn9ThDasA.png\" width=\"350\"/><br>Image source:\n",
        "<a href=\"https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a\">https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a</a>\n",
        "</center>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "par-N39lvb6Y"
      },
      "source": [
        "## Task 3: Train Model\n",
        "Define `train()` to return loss and accuracy.\n",
        "\n",
        "_Points:_ 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE4entSvNc2q",
        "tags": [
          "otter_answer_cell"
        ]
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_total_correct = 0.0\n",
        "    for i, (inputs, labels) in enumerate(tqdm(dataloader)):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        ################YOUR CODE BELOW HERE####################\n",
        "\n",
        "        # Step1: Zero the parameter gradients; Clear off previous weights in order to obtain updated weights.\n",
        "        ...\n",
        "\n",
        "        # Step2: Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Step3: Compute the loss\n",
        "        loss = ...\n",
        "\n",
        "        # Step4: Compute the gradients wrt the loss\n",
        "        ...\n",
        "\n",
        "        # Step5: Use optimizer to update the weights based on the internally stored gradients\n",
        "        ...\n",
        "\n",
        "        ################YOUR CODE ABOVE HERE####################\n",
        "\n",
        "        # Calculate statistics\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Calculate running loss and accuracy\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_total_correct += torch.sum(preds == labels)\n",
        "\n",
        "    # Calculate epoch loss and accuracy\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_accuracy = (running_total_correct / len(dataloader.dataset)) * 100\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2FIXlTUArpSE"
      },
      "outputs": [],
      "source": [
        "# Helper for testing train function (do not modify)\n",
        "def create_mini_loader(batch_size=4, num_batches=2):\n",
        "    \"\"\"Creates a tiny dataloader for testing purposes.\"\"\"\n",
        "    X = torch.randn(batch_size * num_batches, 3, 64, 64)\n",
        "    y = torch.randint(0, 10, (batch_size * num_batches,))\n",
        "    mini_dataset = torch.utils.data.TensorDataset(X, y)\n",
        "    return torch.utils.data.DataLoader(mini_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "zrsys3CerpSE"
      },
      "outputs": [],
      "source": [
        "def test3_public():\n",
        "    hparams = {'activation': 'relu', 'pooling': 'maxpool'}\n",
        "    test_model = baseline(hparams).to(device)\n",
        "    mini_loader = create_mini_loader(batch_size=4, num_batches=2)\n",
        "    test_criterion = nn.CrossEntropyLoss()\n",
        "    test_optimizer = torch.optim.Adam(test_model.parameters(), lr=0.01)\n",
        "    result = train(test_model, mini_loader, test_criterion, test_optimizer)\n",
        "    assert isinstance(result, tuple), f'train() should return a tuple, got {type(result)}'\n",
        "    assert len(result) == 2, f'train() should return 2 values (loss, accuracy), got {len(result)}'\n",
        "    loss, accuracy = result\n",
        "    assert isinstance(loss, float), f'Loss should be a float, got {type(loss)}'\n",
        "    assert loss >= 0, f'Loss should be non-negative, got {loss}'\n",
        "\n",
        "test3_public()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D2XrpGxtJ9w"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvjKlBsNP_pF"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_total_correct = 0.0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(tqdm(dataloader)):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_total_correct += torch.sum(preds == labels)\n",
        "\n",
        "    # Calculate epoch loss and accuracy\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_accuracy = (running_total_correct / len(dataloader.dataset)) * 100\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzTl0FX3tN8m"
      },
      "source": [
        "Putting it all together, we can now commence training and evaluation in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNgDSfQDMSFI"
      },
      "outputs": [],
      "source": [
        "def main(hparams):\n",
        "    # Keep track of the best loss and\n",
        "    # best model weights with the lowest loss\n",
        "    best_loss = np.inf\n",
        "    best_acc = 0.0\n",
        "    best_model = None\n",
        "    #get data\n",
        "    train_loader, valid_loader, test_loader = data_prep(batch_size=hparams['batch_size'],num_workers=hparams['num_workers'],transforms_type=hparams['transforms_type'])\n",
        "    #get model\n",
        "    model = get_model(model_type=hparams['model_type'],hparams=hparams)\n",
        "\n",
        "    # Specify optimizer\n",
        "    if hparams['optimizer'] == 'adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=hparams['learning_rate'])\n",
        "    elif hparams['optimizer'] == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=hparams['learning_rate'])\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    results = {'train_loss':[],'val_loss':[],'train_acc':[],'val_acc':[]}\n",
        "    # Train and test over n_epochs\n",
        "    for epoch in range(hparams['n_epochs']):\n",
        "        print(\"Epoch {}\".format(epoch+1))\n",
        "        train_loss, train_acc = train(model, train_loader,criterion,optimizer)\n",
        "        val_loss, val_acc = evaluate(model, valid_loader, criterion)\n",
        "\n",
        "        results['train_loss'].append(train_loss)\n",
        "        results['val_loss'].append(val_loss)\n",
        "        results['train_acc'].append(train_acc)\n",
        "        results['val_acc'].append(val_acc)\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model = model\n",
        "            best_acc = val_acc\n",
        "\n",
        "    return results, best_acc, best_model, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZB9GftKIDXd"
      },
      "outputs": [],
      "source": [
        "def gather_results(results):\n",
        "    results['train_acc'] = [a.cpu() for a in results['train_acc']]\n",
        "    results['val_acc'] = [a.cpu() for a in results['val_acc']]\n",
        "    x = list(range(len(results['val_acc'])))\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
        "    # plot lines\n",
        "\n",
        "    ax[0].plot(x, results['train_loss'], label = \"train_loss\")\n",
        "    ax[0].plot(x, results['val_loss'], label = \"val_loss\")\n",
        "    ax[0].set_xlabel(\"epoch\")\n",
        "    ax[0].set_ylabel(\"loss\")\n",
        "    ax[0].legend()\n",
        "    ax[0].set_title(\"loss\")\n",
        "\n",
        "    # plot lines\n",
        "    ax[1].plot(x, results['train_acc'], label = \"train_acc\")\n",
        "    ax[1].plot(x, results['val_acc'], label = \"val_acc\")\n",
        "    ax[1].set_xlabel(\"epoch\")\n",
        "    ax[1].set_ylabel(\"accuracy\")\n",
        "    ax[1].legend()\n",
        "    ax[1].set_title(\"accuracy\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14oPIiaqb-hI"
      },
      "source": [
        "## Save Model\n",
        "\n",
        "We can now save the model to our local Google drive as follows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arPoUFyrrpSE"
      },
      "outputs": [],
      "source": [
        "def save_model(hparams, best_model):\n",
        "    model_dir = \"./models/\"\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_file = model_dir + 'best_model.pth'\n",
        "    torch.save(best_model, model_file)\n",
        "    print(hparams['model_type']+' Model successfully saved to {}'.format(model_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeqCoai8hcYr"
      },
      "source": [
        "## Load Model\n",
        "Here we show you how to load the saved model from the previous step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRRJ0vlCheDD"
      },
      "outputs": [],
      "source": [
        "def load_model(hparams):\n",
        "    model_dir = \"./models/\"\n",
        "    model_file = model_dir + 'best_model.pth'\n",
        "    model = torch.load(model_file, map_location=device)\n",
        "    model.eval()\n",
        "    print('Model file {} successfully loaded.'.format(model_file))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "oh9T3GMGrpSF"
      },
      "source": [
        "## Task 4: Hyperparameter search\n",
        "Define a simple hyperparameter search function hyper_sweep() that does grid sampling.\n",
        "\n",
        "_Points:_ 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Gn8VlI513_nf"
      },
      "outputs": [],
      "source": [
        "def get_hparams(model_type='baseline',n_epochs=10,learning_rate=1e-2, optimizer='sgd', activation='relu',pooling='maxpool'):\n",
        "    hparams = {}\n",
        "    hparams['num_workers'] = 0\n",
        "    hparams['batch_size'] = 32\n",
        "    hparams['transforms_type'] = 'original'\n",
        "    hparams['model_type'] = model_type\n",
        "    hparams['n_epochs'] = n_epochs\n",
        "    hparams['learning_rate'] = learning_rate\n",
        "    hparams['optimizer'] = optimizer\n",
        "    hparams['activation'] = activation\n",
        "    hparams['pooling'] = pooling\n",
        "    return hparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ko2uRmXP-hx",
        "tags": [
          "otter_answer_cell"
        ]
      },
      "outputs": [],
      "source": [
        "def hyper_sweep(hyper_params):\n",
        "    \"\"\"\n",
        "    Perform grid search over hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        hyper_params: Dictionary where keys are parameter names and values are lists of values to try\n",
        "\n",
        "    Returns:\n",
        "        results_final: DataFrame containing all hyperparameter combinations and their validation accuracies\n",
        "        best_model_final: The model with the best validation accuracy\n",
        "\n",
        "    Requirements:\n",
        "        - Save results_final to 'hyper_sweep_results.csv' before returning\n",
        "        - The CSV must include a 'val_acc' column with validation accuracies\n",
        "    \"\"\"\n",
        "    ################YOUR CODE BELOW HERE####################\n",
        "    ...\n",
        "    ################YOUR CODE ABOVE HERE####################\n",
        "\n",
        "    # 'results_final' should contain each combination of parameters with its corresponding final validation accuracy\n",
        "    # 'best_model_final' should return the model with the best validation accuracy\n",
        "    return results_final, best_model_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "0TDn1pD3rpSF"
      },
      "outputs": [],
      "source": [
        "# Show hyper_sweep() functionality by running this cell\n",
        "hyper_params={\n",
        "    'model_type': ['baseline'],\n",
        "    'n_epochs': [10],                 #Choose the suitable number of max epochs after watching loss curves\n",
        "    'learning_rate': [1e-2],          #Choose the suitable learning rate after watching loss curves\n",
        "    'optimizers': ['adam'],           #Options: ['sgd','adam']\n",
        "    'activations':['relu'],           #Options: ['relu','tanh']\n",
        "    'pooling':['maxpool', 'avgpool']             #Options: ['maxpool','avgpool']\n",
        "}\n",
        "\n",
        "results_final, best_model_final = hyper_sweep(hyper_params)\n",
        "results_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "KMH_HKGBrpSF"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    results_final.to_csv('hyper_sweep_results.csv', index=False)\n",
        "except NameError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "JoOj1ixErpSF"
      },
      "outputs": [],
      "source": [
        "def test4_public():\n",
        "    csv_path = '/autograder/submission/hyper_sweep_results.csv'\n",
        "    if not os.path.exists(csv_path):\n",
        "        csv_path = 'hyper_sweep_results.csv'\n",
        "\n",
        "    results_df = pd.read_csv(csv_path)\n",
        "    assert 'val_acc' in results_df.columns, \"Results must have 'val_acc' column\"\n",
        "    assert len(results_df) >= 1, \"Results should have at least 1 row\"\n",
        "\n",
        "test4_public()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNo9IfMyN817"
      },
      "source": [
        "# Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVK_TRN9ODTA"
      },
      "outputs": [],
      "source": [
        "# Evaluate your best model on test set\n",
        "evaluate(best_model_final, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIm_ATQ5So-6"
      },
      "source": [
        "# Experiment with model1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsOBOYviSB8g"
      },
      "outputs": [],
      "source": [
        "#Hyper-parameter sweep\n",
        "hyper_params={\n",
        "    'model_type': ['model1'],\n",
        "    'n_epochs': [],                          #Choose the suitable number of max epochs after watching loss curves;pass it as item in a list\n",
        "    'learning_rate': [],                     #Choose the suitable learning rate after watching loss curves;pass it as item in a list\n",
        "    'optimizers': ['sgd','adam'],            #Options: ['sgd','adam']\n",
        "    'activations':['relu','tanh'],           #Options: ['relu','tanh']\n",
        "    'pooling':['maxpool','avgpool']          #Options: ['maxpool','avgpool']\n",
        "}\n",
        "\n",
        "results_final, best_model_final = hyper_sweep(hyper_params)\n",
        "results_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJsAivjASIPq"
      },
      "outputs": [],
      "source": [
        "# Evaluate your best model on test set\n",
        "evaluate(best_model_final, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJEeFst0Sv3h"
      },
      "source": [
        "# Experiment with model2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VwhAEpbSamq"
      },
      "outputs": [],
      "source": [
        "#Hyper-parameter sweep\n",
        "hyper_params={\n",
        "    'model_type': ['model2'],\n",
        "    'n_epochs': [],                          #Choose the suitable number of max epochs after watching loss curves;pass it as item in a list\n",
        "    'learning_rate': [],                     #Choose the suitable learning rate after watching loss curves;pass it as item in a list\n",
        "    'optimizers': ['sgd','adam'],            #Options: ['sgd','adam']\n",
        "    'activations':['relu','tanh'],           #Options: ['relu','tanh']\n",
        "    'pooling':['maxpool','avgpool']          #Options: ['maxpool','avgpool']\n",
        "}\n",
        "\n",
        "results_final, best_model_final = hyper_sweep(hyper_params)\n",
        "results_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr8r9prHSflb"
      },
      "outputs": [],
      "source": [
        "# Evaluate your best model on test set\n",
        "evaluate(best_model_final, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ3ef1a34gix"
      },
      "source": [
        "# Fine-tuning ResNet18\n",
        "\n",
        "We will use a standard neural network architecture, called ResNet18 from the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "\n",
        "Note that when we load the model, we set the `pre-trained` flag to be True to indicate that the loaded model should be already pre-trained on the Imagenet dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ebfenxTpV233"
      },
      "outputs": [],
      "source": [
        "#Config for resnet18\n",
        "hparams = {}\n",
        "hparams['num_workers'] = 2\n",
        "hparams['batch_size'] = 32\n",
        "hparams['transforms_type'] = 'imagenet'\n",
        "hparams['n_epochs'] = 5\n",
        "hparams['learning_rate'] = 1e-3\n",
        "hparams['optimizer'] = 'adam'\n",
        "hparams['model_type'] = 'resnet18'\n",
        "results, best_acc, best_model, test_loader = main(hparams)\n",
        "\n",
        "#gather results\n",
        "gather_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V8Cx_4xWLpB"
      },
      "outputs": [],
      "source": [
        "# Evaluate your best model on test set\n",
        "evaluate(best_model, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "zU1gd_1mrpSG"
      },
      "source": [
        "# Task 5: Evaluate Model\n",
        "Out of the models that you have trained and saved, pick one to be tested for quality. ***40%*** accuracy will pass the quality check.\n",
        "\n",
        "***Hint:*** Use `load_model()` and  `evaluate()` previously defined.\n",
        "\n",
        "_Points:_ 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "otter": {
          "tests": [
            "evaluate"
          ]
        },
        "tags": [
          "otter_answer_cell"
        ],
        "id": "UANHx4NgrpSG"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE GOES HERE\n",
        "...\n",
        "\n",
        "loss, accuracy = ...\n",
        "\n",
        "print(accuracy)\n",
        "\n",
        "# Save your best model\n",
        "save_model(hparams, best_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3D8siKoejsb"
      },
      "source": [
        "# Suggestions\n",
        "\n",
        "\n",
        "*   We have not asked you to sweep through two very important hyper-parameters: `learning_rate` and `n_epochs`. You can choose to do so by first fixing other hyper-parameters and decide on a reasonable values of `learning_rate` and `n_epochs` by observing the loss curves.\n",
        "*   We have demonstrated a simple `baseline` model and a fine-tuned `ResNet18` model for this project. You can compare the performance of your designed models and expect the test performance to fall within the range between these models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmIB3RZhiRlb"
      },
      "source": [
        "# Submission Guide\n",
        "#### NOTE: Submit (1) main.ipynb, (2) best_model.pth and (3) hyper_sweep_results.csv to gradescope"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6uIzSOda2j9"
      },
      "source": [
        "# Credits\n",
        "Adapted from a notebook originally prepared by **Ankur Mahesh** and **Isabelle Tingzon**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYk0TXtZ0G4Y"
      },
      "source": [
        "# References\n",
        "- Helber, P., Bischke, B., Dengel, A., & Borth, D. (2019). Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7), 2217-2226."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X45mN5AwrpSG"
      },
      "source": [
        "# AI Usage Summary\n",
        "\n",
        "[as per course policy, we allow the use of AI assistants as part of completing homework assignments, however such usage must be acknowledged. If it is detected and not acknowledged, it will be considered a violation of academic integrity. As such, please summarize what AI tools you used to complete this assignment, including for understanding the concepts and generating source code. If you used code generation, please explain the extent to which you had to modify the solution for it to function correctly.]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "herb",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "otter": {
      "OK_FORMAT": true,
      "assignment_name": "hw3",
      "tests": {
        "evaluate": {
          "name": "evaluate",
          "points": 6,
          "suites": [
            {
              "cases": [],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "hypersweep": {
          "name": "hypersweep",
          "points": 5,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> def test4_public():\n...     csv_path = 'hyper_sweep_results.csv'\n...     results_df = pd.read_csv(csv_path)\n...     assert 'val_acc' in results_df.columns, \"Results must have 'val_acc' column\"\n...     assert len(results_df) >= 1, 'Results should have at least 1 row'\n>>> test4_public()\n",
                  "hidden": false,
                  "locked": false,
                  "points": 2
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "model1": {
          "name": "model1",
          "points": null,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> def test1_public(hparams1):\n...     model = model1(hparams1)\n...     dummy_input = torch.randn(4, 3, 64, 64)\n...     output = model(dummy_input)\n...     assert output.shape == (4, 10), f'Expected output shape (4, 10), got {output.shape}'\n",
                  "hidden": false,
                  "locked": false,
                  "points": 3
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "model2": {
          "name": "model2",
          "points": null,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> def test2_public():\n...     hparams2 = {'activation': 'relu', 'pooling': 'maxpool'}\n...     model = model2(hparams2)\n...     dummy_input = torch.randn(4, 3, 64, 64)\n...     model.eval()\n...     with torch.no_grad():\n...         output = model(dummy_input)\n...     assert output.shape == (4, 10), f'Expected output shape (4, 10), got {output.shape}'\n>>> test2_public()\n",
                  "hidden": false,
                  "locked": false,
                  "points": 3
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "train": {
          "name": "train",
          "points": null,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> def test3_public():\n...     hparams = {'activation': 'relu', 'pooling': 'maxpool'}\n...     test_model = baseline(hparams).to(device)\n...     mini_loader = create_mini_loader(batch_size=4, num_batches=2)\n...     test_criterion = nn.CrossEntropyLoss()\n...     test_optimizer = torch.optim.Adam(test_model.parameters(), lr=0.01)\n...     result = train(test_model, mini_loader, test_criterion, test_optimizer)\n...     assert isinstance(result, tuple), f'train() should return a tuple, got {type(result)}'\n...     assert len(result) == 2, f'train() should return 2 values (loss, accuracy), got {len(result)}'\n...     (loss, accuracy) = result\n...     assert isinstance(loss, float), f'Loss should be a float, got {type(loss)}'\n...     assert loss >= 0, f'Loss should be non-negative, got {loss}'\n>>> test3_public()\n",
                  "hidden": false,
                  "locked": false,
                  "points": 2
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}